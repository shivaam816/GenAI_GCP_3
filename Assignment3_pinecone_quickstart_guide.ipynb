{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "455ab645",
      "metadata": {
        "id": "455ab645"
      },
      "source": [
        "\n",
        "# Pinecone: Quickstart — Dump & Retrieve with Open‑Source Embeddings\n",
        "\n",
        "This notebook shows how to:\n",
        "- Initialize the Pinecone client (serverless)\n",
        "- Create an index\n",
        "- Embed sample data with a free, open-source model (`sentence-transformers/all-MiniLM-L6-v2`)\n",
        "- Upsert vectors into Pinecone\n",
        "- Run retrieval (vector similarity search)\n",
        "\n",
        "> **Note:** You need a Pinecone API key. Create an account by login with Outlook/google at https://login.pinecone.io/\n",
        "> This notebook uses Pinecone **serverless** (no pods to manage).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed Tutorial and docs: https://docs.pinecone.io/guides/get-started/quickstart"
      ],
      "metadata": {
        "id": "VGZpVSB9Q1rL"
      },
      "id": "VGZpVSB9Q1rL"
    },
    {
      "cell_type": "markdown",
      "id": "e7188ad3",
      "metadata": {
        "id": "e7188ad3"
      },
      "source": [
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Python 3.9+\n",
        "- A Pinecone API key (set as `PINECONE_API_KEY` in your environment)\n",
        "- Internet access (to install packages and call Pinecone)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fb6068e3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb6068e3",
        "outputId": "4192eeaa-e883-4861-9d43-2871ba5e4659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/587.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m583.7/587.6 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/259.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.3/259.3 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/65.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "\n",
        "# If running locally, uncomment to install dependencies.\n",
        "# !pip install --upgrade pip\n",
        "!python -m pip install pinecone sentence-transformers langchain-pinecone langchain langchain-huggingface --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4ea91f72",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ea91f72",
        "outputId": "1e2d6c85-db54-4111-904a-978e974fa9f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/langchain_pinecone/__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Pinecone (serverless)\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c772fbe0",
      "metadata": {
        "id": "c772fbe0"
      },
      "source": [
        "\n",
        "## 1) Configure credentials and client\n",
        "Make sure your API key is available as an environment variable:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if you have already added API key to env file, then this cell is not needed\n",
        "import os\n",
        "os.environ['PINECONE_API_KEY'] = \"pcsk_23rEUe_E7CHN2368jN9x1dE8k4sac9kX6UPQ4h1tLTbBV8HfEXW6DJCVFzKLfKMMdfWcTM\""
      ],
      "metadata": {
        "id": "-DVw4MyTQl4L"
      },
      "id": "-DVw4MyTQl4L",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "e9f1c49f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9f1c49f",
        "outputId": "ff90652a-3eb8-451a-8204-c4821de25623"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PINECONE API KEY found\n"
          ]
        }
      ],
      "source": [
        "\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
        "if not PINECONE_API_KEY:\n",
        "    raise RuntimeError(\"PINECONE_API_KEY not found. Please set it in your environment.\")\n",
        "else:\n",
        "  print(\"PINECONE API KEY found\")\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6a65067",
      "metadata": {
        "id": "a6a65067"
      },
      "source": [
        "\n",
        "## 2) Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can swap model_name to 'sentence-transformers/all-MiniLM-L12-v2',\n",
        "# 'BAAI/bge-small-en-v1.5' etc. If you do, the dimension will be re-detected below.\n",
        "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=MODEL_NAME,\n",
        "    model_kwargs={\"device\": \"cpu\"},                 # change to \"cuda\" if you have a GPU\n",
        "    encode_kwargs={\"normalize_embeddings\": True},   # cosine works well with normalized vectors\n",
        ")\n",
        "\n",
        "# Dynamically detect the embedding dimension\n",
        "test_dim = len(embeddings.embed_query(\"dimension probe\"))"
      ],
      "metadata": {
        "id": "q0-dmSkP1_0a"
      },
      "id": "q0-dmSkP1_0a",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ac6a6c7a",
      "metadata": {
        "id": "ac6a6c7a"
      },
      "source": [
        "\n",
        "## 3) Create (or reuse) a serverless index\n",
        "We will create a small `cosine` index sized for 384‑dimensional vectors (the embedding size of `all-MiniLM-L6-v2`).  \n",
        "Change the name if you want to keep multiple test indexes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "9ef3e57e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ef3e57e",
        "outputId": "2b4c127a-75b7-44da-ba25-339857365b2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating index 'tredenceb3' ...\n",
            "{'dimension': 384,\n",
            " 'index_fullness': 0.0,\n",
            " 'metric': 'cosine',\n",
            " 'namespaces': {},\n",
            " 'total_vector_count': 0,\n",
            " 'vector_type': 'dense'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "INDEX_NAME = \"tredenceb3\" # you can change the name - trial version allows only one index per account, you can login to pinecone and delete the index if needed\n",
        "METRIC = \"cosine\"\n",
        "\n",
        "# Create the index if it doesn't exist\n",
        "existing = [idx[\"name\"] for idx in pc.list_indexes()]\n",
        "if INDEX_NAME not in existing:\n",
        "    print(f\"Creating index '{INDEX_NAME}' ...\")\n",
        "    pc.create_index(\n",
        "        name=INDEX_NAME,\n",
        "        dimension=test_dim,\n",
        "        metric=METRIC,\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),)\n",
        "    # optional: wait a moment for the index to be ready\n",
        "    time.sleep(5)\n",
        "else:\n",
        "    print(f\"Index '{INDEX_NAME}' already exists, reusing it.\")\n",
        "\n",
        "index = pc.Index(INDEX_NAME)\n",
        "print(index.describe_index_stats())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "744b22cb",
      "metadata": {
        "id": "744b22cb"
      },
      "outputs": [],
      "source": [
        "texts = [\n",
        "    \"LangChain helps developers build LLM applications with composable tools and chains.\",\n",
        "    \"FastAPI is a modern, high-performance web framework for building APIs with Python.\",\n",
        "    \"Vector databases store high-dimensional vectors and enable efficient similarity search.\",\n",
        "    \"Pinecone is a fully managed vector database service with serverless indexes.\",\n",
        "    \"Transformers use self-attention to capture long-range dependencies in sequences.\",\n",
        "]\n",
        "metadatas = [\n",
        "    {\"source\": \"docs\",  \"topic\": \"LLM apps\"},\n",
        "    {\"source\": \"docs\",  \"topic\": \"APIs\"},\n",
        "    {\"source\": \"notes\", \"topic\": \"vector db\"},\n",
        "    {\"source\": \"notes\", \"topic\": \"pinecone\"},\n",
        "    {\"source\": \"wiki\",  \"topic\": \"transformers\"},\n",
        "]\n",
        "ids = [f\"doc-{i+1}\" for i in range(len(texts))]  # Optional: control your own IDs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47f27fe4",
      "metadata": {
        "id": "47f27fe4"
      },
      "source": [
        "\n",
        "## 4) Upsert vectors into Pinecone\n",
        "We attach `id`, `values` (the vector), and optional `metadata` per record.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = PineconeVectorStore(\n",
        "    index_name=INDEX_NAME,\n",
        "    embedding=embeddings,\n",
        "    namespace=None,           # set a namespace string if you want to isolate data\n",
        "    pinecone_api_key=PINECONE_API_KEY,  # optional; will default to env var\n",
        ")\n",
        "\n",
        "# Upsert (add) texts into Pinecone via LangChain:\n",
        "vectorstore.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n",
        "print(\"Upsert complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQc4NlsOSYSg",
        "outputId": "d6e4ce25-0b66-4f36-b081-502b0ee7af4d"
      },
      "id": "UQc4NlsOSYSg",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upsert complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d95e406c",
      "metadata": {
        "id": "d95e406c"
      },
      "source": [
        "\n",
        "## 5) Retrieval (semantic search)\n",
        "We will embed a query and search for the top‑k nearest neighbors by cosine similarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "356af0d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "356af0d1",
        "outputId": "b2c44153-9cc4-4b11-a9eb-597aaeb345b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 3 results for query: 'How do I build applications with large language models?'\n",
            "\n",
            "[1] score=0.4255\n",
            "   text   : LangChain helps developers build LLM applications with composable tools and chains.\n",
            "   meta   : {'source': 'docs', 'topic': 'LLM apps'}\n",
            "\n",
            "[2] score=0.3577\n",
            "   text   : FastAPI is a modern, high-performance web framework for building APIs with Python.\n",
            "   meta   : {'source': 'docs', 'topic': 'APIs'}\n",
            "\n",
            "[3] score=0.1905\n",
            "   text   : Transformers use self-attention to capture long-range dependencies in sequences.\n",
            "   meta   : {'source': 'wiki', 'topic': 'transformers'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query = \"How do I build applications with large language models?\"\n",
        "k = 3\n",
        "\n",
        "# Get documents and scores:\n",
        "docs_and_scores = vectorstore.similarity_search_with_score(query, k=k)\n",
        "\n",
        "print(f\"\\nTop {k} results for query: {query!r}\\n\")\n",
        "for rank, (doc, score) in enumerate(docs_and_scores, start=1):\n",
        "    # 'doc' is a LangChain Document with .page_content and .metadata\n",
        "    print(f\"[{rank}] score={score:.4f}\")\n",
        "    print(\"   text   :\", doc.page_content)\n",
        "    print(\"   meta   :\", doc.metadata)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57798a8d",
      "metadata": {
        "id": "57798a8d"
      },
      "source": [
        "\n",
        "## (Optional) 6) Clean up\n",
        "Uncomment to delete the index when done.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a880435",
      "metadata": {
        "id": "1a880435"
      },
      "outputs": [],
      "source": [
        "\n",
        "# pc.delete_index(INDEX_NAME)\n",
        "# print(f\"Deleted index: {INDEX_NAME}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0511352e",
      "metadata": {
        "id": "0511352e"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "### Troubleshooting Tips\n",
        "\n",
        "- **Auth**: Ensure `PINECONE_API_KEY` is set (and valid).\n",
        "- **Region**: If your account is set to a specific region/cloud, adjust `ServerlessSpec(cloud, region)` accordingly.\n",
        "- **Model**: If you prefer another open-source embedding model (e.g., `all-MiniLM-L12-v2`, `bge-small-en`), just swap it and update `DIMENSION` to match.\n",
        "- **Throughput**: For larger data, batch your upserts and consider concurrency with backoff.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}