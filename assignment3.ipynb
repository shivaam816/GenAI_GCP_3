{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cded95f0",
   "metadata": {},
   "source": [
    "Install all the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80dff6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pinecone sentence-transformers langchain-pinecone langchain langgraph langchain-google-vertexai langchain-huggingface pandas --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8250209d",
   "metadata": {},
   "source": [
    "Import all the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c92ad423",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zadmin/genai/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/zadmin/genai/lib/python3.11/site-packages/langchain_pinecone/__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, Any\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_google_vertexai import VertexAIEmbeddings, VertexAI\n",
    "from google import genai\n",
    "from google.genai.types import HttpOptions\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46213720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded KB entries: 30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_snippet</th>\n",
       "      <th>source</th>\n",
       "      <th>confidence_indicator</th>\n",
       "      <th>last_updated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KB001</td>\n",
       "      <td>What are best practices for debugging?</td>\n",
       "      <td>When addressing debugging, it's important to f...</td>\n",
       "      <td>debugging_guide.md</td>\n",
       "      <td>moderate</td>\n",
       "      <td>2024-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KB002</td>\n",
       "      <td>What are best practices for performance tuning?</td>\n",
       "      <td>When addressing performance tuning, it's impor...</td>\n",
       "      <td>performance tuning_guide.md</td>\n",
       "      <td>moderate</td>\n",
       "      <td>2024-02-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KB003</td>\n",
       "      <td>What are best practices for caching?</td>\n",
       "      <td>When addressing caching, it's important to fol...</td>\n",
       "      <td>caching_guide.md</td>\n",
       "      <td>moderate</td>\n",
       "      <td>2024-03-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KB004</td>\n",
       "      <td>What are best practices for asynchronous progr...</td>\n",
       "      <td>When addressing asynchronous programming, it's...</td>\n",
       "      <td>asynchronous programming_guide.md</td>\n",
       "      <td>moderate</td>\n",
       "      <td>2024-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KB005</td>\n",
       "      <td>What are best practices for API versioning?</td>\n",
       "      <td>When addressing API versioning, it's important...</td>\n",
       "      <td>API versioning_guide.md</td>\n",
       "      <td>moderate</td>\n",
       "      <td>2024-05-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc_id                                           question  \\\n",
       "0  KB001             What are best practices for debugging?   \n",
       "1  KB002    What are best practices for performance tuning?   \n",
       "2  KB003               What are best practices for caching?   \n",
       "3  KB004  What are best practices for asynchronous progr...   \n",
       "4  KB005        What are best practices for API versioning?   \n",
       "\n",
       "                                      answer_snippet  \\\n",
       "0  When addressing debugging, it's important to f...   \n",
       "1  When addressing performance tuning, it's impor...   \n",
       "2  When addressing caching, it's important to fol...   \n",
       "3  When addressing asynchronous programming, it's...   \n",
       "4  When addressing API versioning, it's important...   \n",
       "\n",
       "                              source confidence_indicator last_updated  \n",
       "0                 debugging_guide.md             moderate   2024-01-10  \n",
       "1        performance tuning_guide.md             moderate   2024-02-10  \n",
       "2                   caching_guide.md             moderate   2024-03-10  \n",
       "3  asynchronous programming_guide.md             moderate   2024-04-10  \n",
       "4            API versioning_guide.md             moderate   2024-05-10  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Dataset\n",
    "\n",
    "with open(\"self_critique_loop_dataset.json\", \"r\") as f:\n",
    "    kb_data = json.load(f)\n",
    "\n",
    "print(\"Loaded KB entries:\", len(kb_data))\n",
    "pd.DataFrame(kb_data).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88d9ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have already added API key to env file, then this cell is not needed\n",
    "\n",
    "os.environ['PINECONE_API_KEY'] = \"pcsk_23rEUe_E7CHN2368jN9x1dE8k4sac9kX6UPQ4h1tLTbBV8HfEXW6DJCVFzKLfKMMdfWcTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99ee80cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINECONE API KEY found\n"
     ]
    }
   ],
   "source": [
    "# Embeddings & Pinecone Indexing referred from Assignment3_pinecone_quickstart_guide.ipynb\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "if not PINECONE_API_KEY:\n",
    "    raise RuntimeError(\"PINECONE_API_KEY not found. Please set it in your environment.\")\n",
    "else:\n",
    "  print(\"PINECONE API KEY found\")\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feb70c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pc.delete_index(\"assignment3-shivam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a20f4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "    {\n",
       "        \"name\": \"tredenceb3\",\n",
       "        \"metric\": \"cosine\",\n",
       "        \"host\": \"tredenceb3-dqz56te.svc.aped-4627-b74a.pinecone.io\",\n",
       "        \"spec\": {\n",
       "            \"serverless\": {\n",
       "                \"cloud\": \"aws\",\n",
       "                \"region\": \"us-east-1\"\n",
       "            }\n",
       "        },\n",
       "        \"status\": {\n",
       "            \"ready\": true,\n",
       "            \"state\": \"Ready\"\n",
       "        },\n",
       "        \"vector_type\": \"dense\",\n",
       "        \"dimension\": 384,\n",
       "        \"deletion_protection\": \"disabled\",\n",
       "        \"tags\": null\n",
       "    }\n",
       "]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3abdba92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tredenceb3']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_indexes = [idx[\"name\"] for idx in pc.list_indexes()]\n",
    "existing_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3be6723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zadmin/genai/lib/python3.11/site-packages/vertexai/_model_garden/_model_garden_models.py:278: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
      "  warning_logs.show_deprecation_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension detected: 3072\n",
      "Index 'assignment3-shivam' created.\n",
      "Upserted 30 docs into Pinecone index assignment3-shivam\n"
     ]
    }
   ],
   "source": [
    "INDEX_NAME = \"assignment3-shivam\"\n",
    "\n",
    "existing_indexes = [idx[\"name\"] for idx in pc.list_indexes()]\n",
    "\n",
    "# Embeddings\n",
    "# embedding_model = VertexAIEmbeddings(model_name=\"gemini-embedding-001\")\n",
    "# pip install -U langchain-google-vertexai\n",
    "\n",
    "embedding_model = VertexAIEmbeddings(model_name=\"gemini-embedding-001\")\n",
    "\n",
    "# Dynamically detect embedding dimension\n",
    "VECTOR_DIM = len(embedding_model.embed_query(\"dimension probe\"))\n",
    "print(f\"Embedding dimension detected: {VECTOR_DIM}\")\n",
    "\n",
    "if INDEX_NAME not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=INDEX_NAME,\n",
    "        dimension=VECTOR_DIM,\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "    print(f\"Index '{INDEX_NAME}' created.\")\n",
    "else:\n",
    "    print(f\"Index '{INDEX_NAME}' already exists, reusing it.\")\n",
    "\n",
    "# Prepare texts & metadata\n",
    "texts, metadatas, ids = [], [], []\n",
    "for entry in kb_data:\n",
    "    text = entry.get(\"answer_snippet\") or entry.get(\"text\") or entry.get(\"question\",\"\")\n",
    "    doc_id = entry.get(\"doc_id\") or entry.get(\"id\") or f\"KB_{len(ids)+1}\"\n",
    "    meta = {\n",
    "        \"doc_id\": doc_id,\n",
    "        \"question\": entry.get(\"question\",\"\"),\n",
    "        \"source\": entry.get(\"source\",\"\"),\n",
    "        \"last_updated\": entry.get(\"last_updated\",\"\")\n",
    "    }\n",
    "    texts.append(text)\n",
    "    metadatas.append(meta)\n",
    "    ids.append(doc_id)\n",
    "\n",
    "# Vectorstore (LangChain-Pinecone wrapper from quickstart)\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index_name=INDEX_NAME,\n",
    "    embedding=embedding_model,\n",
    "    namespace=None,\n",
    "    pinecone_api_key=PINECONE_API_KEY,\n",
    ")\n",
    "\n",
    "# Upsert KB\n",
    "vectorstore.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n",
    "print(f\"Upserted {len(ids)} docs into Pinecone index {INDEX_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e6f4487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models (sample):\n",
      "- models/gemini-2.5-flash\n",
      "- models/gemini-2.5-pro\n",
      "- models/gemini-2.0-flash\n",
      "- models/gemini-2.0-flash-001\n",
      "- models/gemini-2.0-flash-lite-001\n",
      "- models/gemini-2.0-flash-lite\n",
      "- models/gemini-2.0-flash-preview-image-generation\n",
      "- models/gemini-2.5-flash-lite\n",
      "- models/embedding-001\n",
      "- models/text-embedding-004\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No Gemini 1.5 / Gemini Pro model found in this environment. Please enable a model.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m     DEFAULT_MODEL = [m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m available_models \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgemini-pro\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m m][\u001b[32m0\u001b[39m]\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# if nothing found, raise so user configures model availability\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo Gemini 1.5 / Gemini Pro model found in this environment. Please enable a model.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mUsing model:\u001b[39m\u001b[33m\"\u001b[39m, DEFAULT_MODEL)\n",
      "\u001b[31mRuntimeError\u001b[39m: No Gemini 1.5 / Gemini Pro model found in this environment. Please enable a model."
     ]
    }
   ],
   "source": [
    "# List available models in this project/region and pick the best \"Gemini 1.5\" variant if present.\n",
    "\n",
    "\n",
    "client = genai.Client(http_options=HttpOptions(api_version=\"v1\"))\n",
    "available_models = [m.name for m in client.models.list()]\n",
    "print(\"Available models (sample):\")\n",
    "for m in available_models:\n",
    "    print(\"-\", m)\n",
    "\n",
    "# Assignment requests Gemini 1.5 -> prefer exact gemini-1.5 if present, otherwise pro/flash previews, otherwise gemini-pro\n",
    "if any(m == \"gemini-1.5\" for m in available_models):\n",
    "    DEFAULT_MODEL = \"gemini-1.5\"\n",
    "elif any(\"gemini-1.5-pro\" in m for m in available_models):\n",
    "    DEFAULT_MODEL = [m for m in available_models if \"gemini-1.5-pro\" in m][0]  # pick matching name\n",
    "elif any(\"gemini-1.5-flash\" in m for m in available_models):\n",
    "    DEFAULT_MODEL = [m for m in available_models if \"gemini-1.5-flash\" in m][0]\n",
    "elif any(\"gemini-pro\" in m for m in available_models):\n",
    "    DEFAULT_MODEL = [m for m in available_models if \"gemini-pro\" in m][0]\n",
    "else:\n",
    "    # if nothing found, raise so user configures model availability\n",
    "    raise RuntimeError(\"No Gemini 1.5 / Gemini Pro model found in this environment. Please enable a model.\")\n",
    "print(\"\\nUsing model:\", DEFAULT_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cdcf4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph Workflow\n",
    "\n",
    "llm = VertexAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "\n",
    "class RAGState(Dict[str, Any]):\n",
    "    question: str\n",
    "    retrieved_docs: list\n",
    "    answer: str\n",
    "    critique: str\n",
    "\n",
    "# Phase 1: Retrieve\n",
    "def retrieve_kb(state: RAGState):\n",
    "    docs = vectorstore.similarity_search(state[\"question\"], k=5)\n",
    "    state[\"retrieved_docs\"] = [d.page_content for d in docs]\n",
    "    return state\n",
    "\n",
    "# Phase 2: Generate\n",
    "def generate_answer(state: RAGState):\n",
    "    context = \"\\n\".join(state[\"retrieved_docs\"])\n",
    "    prompt = f\"\"\"\n",
    "    Question: {state['question']}\n",
    "    Context (KB): {context}\n",
    "    Provide an answer citing doc_ids as [KBxxx].\n",
    "    \"\"\"\n",
    "    state[\"answer\"] = llm(prompt)\n",
    "    return state\n",
    "\n",
    "# Phase 3: Critique\n",
    "def critique_answer(state: RAGState):\n",
    "    prompt = f\"\"\"\n",
    "    Review this answer:\n",
    "    {state['answer']}\n",
    "    Decide if it's COMPLETE or needs refinement.\n",
    "    Respond with either:\n",
    "    COMPLETE\n",
    "    REFINE: <keywords>\n",
    "    \"\"\"\n",
    "    state[\"critique\"] = llm(prompt)\n",
    "    return state\n",
    "\n",
    "# Phase 4: Refine\n",
    "def refine_answer(state: RAGState):\n",
    "    if state[\"critique\"].startswith(\"REFINE\"):\n",
    "        keywords = state[\"critique\"].replace(\"REFINE:\", \"\").strip()\n",
    "        docs = vectorstore.similarity_search(keywords, k=1)\n",
    "        if docs:\n",
    "            state[\"retrieved_docs\"].append(docs[0].page_content)\n",
    "        context = \"\\n\".join(state[\"retrieved_docs\"])\n",
    "        prompt = f\"\"\"\n",
    "        Question: {state['question']}\n",
    "        Context (KB): {context}\n",
    "        Provide a refined answer citing doc_ids as [KBxxx].\n",
    "        \"\"\"\n",
    "        state[\"answer\"] = llm(prompt)\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2f0a94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LangGraph\n",
    "\n",
    "graph = StateGraph(RAGState)\n",
    "\n",
    "graph.add_node(\"retrieve_kb\", retrieve_kb)\n",
    "graph.add_node(\"generate_answer\", generate_answer)\n",
    "graph.add_node(\"critique_answer\", critique_answer)\n",
    "graph.add_node(\"refine_answer\", refine_answer)\n",
    "\n",
    "graph.set_entry_point(\"retrieve_kb\")\n",
    "graph.add_edge(\"retrieve_kb\", \"generate_answer\")\n",
    "graph.add_edge(\"generate_answer\", \"critique_answer\")\n",
    "\n",
    "def critique_condition(state: RAGState):\n",
    "    return \"refine_answer\" if state[\"critique\"].startswith(\"REFINE\") else END\n",
    "\n",
    "graph.add_conditional_edges(\"critique_answer\", critique_condition)\n",
    "graph.add_edge(\"refine_answer\", END)\n",
    "\n",
    "compiled_graph = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f430853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8246/1710512451.py:25: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  state[\"answer\"] = llm(prompt)\n"
     ]
    }
   ],
   "source": [
    "# Test Queries\n",
    "\n",
    "test_queries = [\n",
    "    \"What are best practices for caching?\",\n",
    "    \"How should I set up CI/CD pipelines?\",\n",
    "    \"What are performance tuning tips?\",\n",
    "    \"How do I version my APIs?\",\n",
    "    \"What should I consider for error handling?\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "for q in test_queries:\n",
    "    final_state = compiled_graph.invoke({\"question\": q})\n",
    "    results.append({\n",
    "        \"question\": q,\n",
    "        \"answer\": final_state[\"answer\"],\n",
    "        \"critique\": final_state[\"critique\"]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7484f49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     question  \\\n",
      "0        What are best practices for caching?   \n",
      "1        How should I set up CI/CD pipelines?   \n",
      "2           What are performance tuning tips?   \n",
      "3                   How do I version my APIs?   \n",
      "4  What should I consider for error handling?   \n",
      "\n",
      "                                              answer  \\\n",
      "0  When addressing caching, it's important to fol...   \n",
      "1  When setting up CI/CD pipelines, it's importan...   \n",
      "2  When addressing performance tuning, it's impor...   \n",
      "3  To version your APIs, it's important to follow...   \n",
      "4  When considering error handling, it's importan...   \n",
      "\n",
      "                                            critique  \n",
      "0  REFINE: Caching patterns (e.g., Cache-Aside, R...  \n",
      "1            REFINE: specificity, benefits, examples  \n",
      "2                                           COMPLETE  \n",
      "3  REFINE: API versioning patterns, URL versionin...  \n",
      "4  REFINE: `justification`, `specificity of patte...  \n",
      "Results saved to assignment3_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "print(results_df)\n",
    "results_df.to_csv(\"assignment3_results.csv\", index=False)\n",
    "print(\"Results saved to assignment3_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895c1a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
